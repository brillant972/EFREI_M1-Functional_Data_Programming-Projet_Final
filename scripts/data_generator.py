#!/usr/bin/env python3
"""
====================================================================================
DATA GENERATOR - UBER EATS RESTAURANT ANALYSIS PROJECT
====================================================================================

Ce script automatise la cr√©ation de la database, des tables, et l'insertion
des donn√©es r√©alistes dans PostgreSQL.

Usage:
    python data_generator.py [--sample N] [--validate]
"""

import pandas as pd
import psycopg2
from psycopg2.extensions import ISOLATION_LEVEL_AUTOCOMMIT
from faker import Faker
import random
from datetime import datetime, timedelta
import os
import logging
import argparse

# Configuration logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Configuration PostgreSQL
POSTGRES_CONFIG = {
    'host': 'localhost',
    'port': 54876,
    'user': 'postgres',
    'password': 'postgres'
}

DATABASE_NAME = 'uber_eats_analysis'

def execute_sql_file(cursor, filepath):
    """Ex√©cute un fichier SQL"""
    try:
        with open(filepath, 'r', encoding='utf-8') as file:
            sql_content = file.read()
            
        # Nettoyer et s√©parer les commandes SQL
        sql_commands = []
        current_command = ""
        
        for line in sql_content.split('\n'):
            line = line.strip()
            if line and not line.startswith('--'):
                current_command += " " + line
                if line.endswith(';'):
                    sql_commands.append(current_command.strip())
                    current_command = ""
        
        # Ajouter la derni√®re commande si elle n'√©tait pas termin√©e par ;
        if current_command.strip():
            sql_commands.append(current_command.strip())
            
        for command in sql_commands:
            if command:
                logger.info(f"Ex√©cution: {command[:50]}...")
                cursor.execute(command)
                
        logger.info(f"Fichier {filepath} ex√©cut√© avec succ√®s")
        return True
        
    except Exception as e:
        logger.error(f"Erreur lors de l'ex√©cution de {filepath}: {e}")
        return False

def setup_database():
    """Cr√©e la database et les tables automatiquement"""
    logger.info("=== SETUP DATABASE ET TABLES ===")
    
    # √âtape 1: Cr√©er la database
    try:
        conn = psycopg2.connect(**POSTGRES_CONFIG)
        conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)
        cursor = conn.cursor()
        
        # V√©rifier si la database existe d√©j√†
        cursor.execute("SELECT 1 FROM pg_catalog.pg_database WHERE datname = %s", (DATABASE_NAME,))
        exists = cursor.fetchone()
        
        if not exists:
            logger.info(f"Cr√©ation de la database {DATABASE_NAME}")
            cursor.execute(f'CREATE DATABASE "{DATABASE_NAME}"')
        else:
            logger.info(f"Database {DATABASE_NAME} existe d√©j√†")
            
        cursor.close()
        conn.close()
        
    except Exception as e:
        logger.error(f"Erreur lors de la cr√©ation de la database: {e}")
        return False
    
    # √âtape 2: Cr√©er les tables
    try:
        db_config = POSTGRES_CONFIG.copy()
        db_config['database'] = DATABASE_NAME
        
        conn = psycopg2.connect(**db_config)
        conn.set_isolation_level(ISOLATION_LEVEL_AUTOCOMMIT)
        cursor = conn.cursor()
        
        # Ex√©cuter le script de cr√©ation des tables
        tables_script = os.path.join(os.path.dirname(__file__), '02_create_tables.sql')
        if execute_sql_file(cursor, tables_script):
            logger.info("Tables cr√©√©es avec succ√®s")
        else:
            logger.error("√âchec de la cr√©ation des tables")
            return False
            
        cursor.close()
        conn.close()
        return True
        
    except Exception as e:
        logger.error(f"Erreur lors de la cr√©ation des tables: {e}")
        return False

def load_restaurant_ids(csv_path, sample_size=None):
    """Charge les restaurant_ids depuis le CSV"""
    try:
        logger.info(f"Lecture du fichier restaurants: {csv_path}")
        df = pd.read_csv(csv_path)
        
        restaurant_ids = df['id'].dropna().astype(int).tolist()
        
        if sample_size:
            restaurant_ids = restaurant_ids[:sample_size]
            logger.info(f"√âchantillon de {len(restaurant_ids)} restaurants")
        else:
            logger.info(f"Total {len(restaurant_ids)} restaurants trouv√©s")
            
        return restaurant_ids
        
    except Exception as e:
        logger.error(f"Erreur lors de la lecture du CSV: {e}")
        return []

def generate_ratings_history(restaurant_id, fake):
    """G√©n√®re l'historique des ratings pour un restaurant"""
    ratings = []
    base_date = datetime.now() - timedelta(days=365)
    
    # G√©n√©rer 8-12 mesures sur 12 mois
    num_measurements = random.randint(8, 12)
    
    for i in range(num_measurements):
        measurement_date = base_date + timedelta(days=random.randint(0, 365))
        rating = round(random.uniform(3.5, 5.0), 2)
        review_count = random.randint(10, 500)
        quarter = f"Q{((measurement_date.month - 1) // 3) + 1}-{measurement_date.year}"
        year_month = measurement_date.strftime("%Y-%m")
        
        ratings.append((
            restaurant_id, rating, review_count, measurement_date,
            quarter, year_month
        ))
    
    return ratings

def generate_delivery_performance(restaurant_id, fake):
    """G√©n√®re les m√©triques de performance de livraison"""
    avg_delivery_time = random.randint(15, 45)
    success_rate = round(random.uniform(85.0, 99.0), 2)
    
    # Heures de pointe r√©alistes
    peak_hours_options = [
        "12:00-13:00,19:00-21:00",
        "11:30-13:30,18:30-20:30",
        "12:00-14:00,19:00-22:00"
    ]
    peak_hours = random.choice(peak_hours_options)
    
    delivery_radius = round(random.uniform(2.5, 15.0), 2)
    last_updated = fake.date_between(start_date='-30d', end_date='today')
    
    return (restaurant_id, avg_delivery_time, success_rate, peak_hours, 
            delivery_radius, last_updated)

def generate_promotional_campaigns(restaurant_id, fake):
    """G√©n√®re les campagnes promotionnelles"""
    campaigns = []
    num_campaigns = random.randint(1, 3)
    
    campaign_names = [
        "Summer Sale", "New Customer Welcome", "Weekend Special",
        "Happy Hour", "Lunch Deal", "Holiday Promotion"
    ]
    
    audiences = ["new_customers", "loyal", "all"]
    
    for _ in range(num_campaigns):
        campaign_name = random.choice(campaign_names)
        discount = round(random.uniform(5.0, 30.0), 2)
        
        start_date = fake.date_between(start_date='-90d', end_date='today')
        end_date = fake.date_between(start_date=start_date, end_date='+30d')
        
        target_audience = random.choice(audiences)
        effectiveness = round(random.uniform(15.0, 85.0), 2)
        
        campaigns.append((
            restaurant_id, campaign_name, discount, start_date,
            end_date, target_audience, effectiveness
        ))
    
    return campaigns

def insert_data_batch(cursor, table_name, columns, data_batch):
    """Ins√®re un batch de donn√©es"""
    if not data_batch:
        return
        
    placeholders = ','.join(['%s'] * len(columns))
    query = f"INSERT INTO {table_name} ({','.join(columns)}) VALUES ({placeholders})"
    
    cursor.executemany(query, data_batch)
    logger.info(f"Ins√©r√© {len(data_batch)} enregistrements dans {table_name}")

def generate_and_insert_data(restaurant_ids, sample_size=None):
    """G√©n√®re et ins√®re toutes les donn√©es"""
    logger.info("=== G√âN√âRATION ET INSERTION DES DONN√âES ===")
    
    fake = Faker()
    
    try:
        db_config = POSTGRES_CONFIG.copy()
        db_config['database'] = DATABASE_NAME
        
        conn = psycopg2.connect(**db_config)
        cursor = conn.cursor()
        
        # Nettoyer les tables existantes
        cursor.execute("TRUNCATE restaurant_ratings_history, delivery_performance, promotional_campaigns RESTART IDENTITY")
        logger.info("Tables nettoy√©es")
        
        # Pr√©parer les batches
        ratings_batch = []
        delivery_batch = []
        campaigns_batch = []
        
        restaurants_to_process = restaurant_ids[:sample_size] if sample_size else restaurant_ids
        
        for i, restaurant_id in enumerate(restaurants_to_process):
            if i % 100 == 0:
                logger.info(f"Traitement restaurant {i+1}/{len(restaurants_to_process)}")
            
            # G√©n√©rer donn√©es pour ce restaurant
            ratings_batch.extend(generate_ratings_history(restaurant_id, fake))
            delivery_batch.append(generate_delivery_performance(restaurant_id, fake))
            campaigns_batch.extend(generate_promotional_campaigns(restaurant_id, fake))
            
            # Ins√©rer par batches de 1000
            if len(ratings_batch) >= 1000:
                insert_data_batch(cursor, "restaurant_ratings_history",
                    ["restaurant_id", "rating", "review_count", "measurement_date", "quarter", "year_month"],
                    ratings_batch)
                ratings_batch = []
                
        # Ins√©rer les derniers batches
        if ratings_batch:
            insert_data_batch(cursor, "restaurant_ratings_history",
                ["restaurant_id", "rating", "review_count", "measurement_date", "quarter", "year_month"],
                ratings_batch)
                
        insert_data_batch(cursor, "delivery_performance",
            ["restaurant_id", "avg_delivery_time_minutes", "delivery_success_rate", 
             "peak_hours", "delivery_zone_radius_km", "last_updated"],
            delivery_batch)
            
        insert_data_batch(cursor, "promotional_campaigns",
            ["restaurant_id", "campaign_name", "discount_percent", "start_date",
             "end_date", "target_audience", "effectiveness_score"],
            campaigns_batch)
        
        conn.commit()
        cursor.close()
        conn.close()
        
        logger.info("‚úÖ Toutes les donn√©es ont √©t√© ins√©r√©es avec succ√®s")
        return True
        
    except Exception as e:
        logger.error(f"Erreur lors de l'insertion des donn√©es: {e}")
        return False

def validate_data():
    """Valide les donn√©es ins√©r√©es"""
    logger.info("=== VALIDATION DES DONN√âES ===")
    
    try:
        db_config = POSTGRES_CONFIG.copy()
        db_config['database'] = DATABASE_NAME
        
        conn = psycopg2.connect(**db_config)
        cursor = conn.cursor()
        
        # Compter les enregistrements
        cursor.execute("SELECT COUNT(*) FROM restaurant_ratings_history")
        ratings_count = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM delivery_performance")
        delivery_count = cursor.fetchone()[0]
        
        cursor.execute("SELECT COUNT(*) FROM promotional_campaigns")
        campaigns_count = cursor.fetchone()[0]
        
        logger.info(f"üìä Statistiques des donn√©es:")
        logger.info(f"   - Ratings history: {ratings_count:,} enregistrements")
        logger.info(f"   - Delivery performance: {delivery_count:,} enregistrements")
        logger.info(f"   - Promotional campaigns: {campaigns_count:,} enregistrements")
        
        cursor.close()
        conn.close()
        
        return True
        
    except Exception as e:
        logger.error(f"Erreur lors de la validation: {e}")
        return False

def main():
    """Fonction principale"""
    parser = argparse.ArgumentParser(description='G√©n√©rateur de donn√©es PostgreSQL')
    parser.add_argument('--sample', type=int, help='Nombre de restaurants √† traiter (pour tests)')
    parser.add_argument('--validate', action='store_true', help='Valider les donn√©es apr√®s insertion')
    
    args = parser.parse_args()
    
    logger.info("üöÄ D√âMARRAGE DU G√âN√âRATEUR DE DONN√âES")
    
    # √âtape 1: Setup database et tables
    if not setup_database():
        logger.error("‚ùå √âchec du setup de la database")
        return
      # √âtape 2: Charger les restaurant IDs
    csv_path = os.path.join(os.path.dirname(__file__), '..', '..', 'restaurants.csv')
    restaurant_ids = load_restaurant_ids(csv_path, args.sample)
    
    if not restaurant_ids:
        logger.error("‚ùå Aucun restaurant trouv√© dans le CSV")
        return
    
    # √âtape 3: G√©n√©rer et ins√©rer les donn√©es
    if not generate_and_insert_data(restaurant_ids, args.sample):
        logger.error("‚ùå √âchec de la g√©n√©ration des donn√©es")
        return
    
    # √âtape 4: Validation optionnelle
    if args.validate:
        validate_data()
    
    logger.info("üéâ G√âN√âRATION TERMIN√âE AVEC SUCC√àS")

if __name__ == "__main__":
    main()
