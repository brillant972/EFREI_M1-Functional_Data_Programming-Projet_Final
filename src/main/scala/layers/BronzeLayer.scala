package layers

import org.apache.spark.sql.{DataFrame, SparkSession}
import utils.{Reader, Writer, ConfigManager}
import com.typesafe.scalalogging.LazyLogging

/**
 * Couche Bronze - Ingestion des donnÃ©es brutes
 * Lecture depuis CSV et PostgreSQL, Ã©criture en Parquet
 */
object BronzeLayer extends LazyLogging {
  /**
   * Extraction des restaurants depuis CSV
   */
  def extractRestaurantsFromCSV(spark: SparkSession): DataFrame = {
    implicit val sparkSession: SparkSession = spark
    logger.info("ğŸ½ï¸ Extraction des restaurants depuis CSV")
    val df = Reader.sourceFromCsv(ConfigManager.restaurantsCsvPath)
    logger.info(s"âœ… ${df.count()} restaurants extraits")
    df
  }

  /**
   * Extraction des menus depuis CSV
   */
  def extractMenusFromCSV(spark: SparkSession): DataFrame = {
    implicit val sparkSession: SparkSession = spark
    logger.info("ğŸ“‹ Extraction des menus depuis CSV")
    val df = Reader.sourceFromCsv(ConfigManager.menusCsvPath)
    logger.info(s"âœ… ${df.count()} items de menu extraits")
    df
  }

  /**
   * Extraction des donnÃ©es temporelles depuis PostgreSQL
   */
  def extractRatingsFromPostgreSQL(spark: SparkSession): DataFrame = {
    implicit val sparkSession: SparkSession = spark
    logger.info("ğŸ“Š Extraction historique ratings depuis PostgreSQL")
    val df = Reader.sourceFromPostgreSQL(ConfigManager.ratingsHistoryTable)
    logger.info(s"âœ… ${df.count()} enregistrements ratings extraits")
    df
  }

  def extractDeliveryFromPostgreSQL(spark: SparkSession): DataFrame = {
    implicit val sparkSession: SparkSession = spark
    logger.info("ğŸšš Extraction performance delivery depuis PostgreSQL")
    val df = Reader.sourceFromPostgreSQL(ConfigManager.deliveryPerformanceTable)
    logger.info(s"âœ… ${df.count()} enregistrements delivery extraits")
    df
  }

  def extractCampaignsFromPostgreSQL(spark: SparkSession): DataFrame = {
    implicit val sparkSession: SparkSession = spark
    logger.info("ğŸ“¢ Extraction campagnes promotionnelles depuis PostgreSQL")
    val df = Reader.sourceFromPostgreSQL(ConfigManager.promotionalCampaignsTable)
    logger.info(s"âœ… ${df.count()} campagnes extraites")
    df
  }

  /**
   * Ã‰criture en format Parquet dans la couche Bronze
   */
  def writeToParquet(df: DataFrame, tableName: String): Unit = {
    val outputPath = s"${ConfigManager.bronzePath}/$tableName/"
    logger.info(s"ğŸ’¾ Ã‰criture $tableName vers $outputPath")
    
    val success = Writer.writeToParquet(df, outputPath)
    if (success) {
      logger.info(s"âœ… $tableName Ã©crit avec succÃ¨s en Bronze")
    } else {
      logger.error(s"âŒ Ã‰chec Ã©criture $tableName en Bronze")
    }
  }

  /**
   * Pipeline complet de la couche Bronze
   */
  def run(spark: SparkSession): Unit = {
    logger.info("ğŸ¥‰ === DÃ‰BUT COUCHE BRONZE ===")
    
    try {
      // ==============================
      //  EXTRACT depuis CSV
      // ==============================
      val restaurantsDF = extractRestaurantsFromCSV(spark)
      val menusDF = extractMenusFromCSV(spark)
      
      // ==============================
      //  EXTRACT depuis PostgreSQL
      // ==============================
      val ratingsDF = extractRatingsFromPostgreSQL(spark)
      val deliveryDF = extractDeliveryFromPostgreSQL(spark)
      val campaignsDF = extractCampaignsFromPostgreSQL(spark)
      
      // ==============================
      //  LOAD vers Bronze (Parquet)
      // ==============================
      writeToParquet(restaurantsDF, "restaurants")
      writeToParquet(menusDF, "menu_items")
      writeToParquet(ratingsDF, "ratings_history")
      writeToParquet(deliveryDF, "delivery_performance")
      writeToParquet(campaignsDF, "promotional_campaigns")
      
      logger.info("ğŸ¥‰ === COUCHE BRONZE TERMINÃ‰E AVEC SUCCÃˆS ===")
      
    } catch {
      case e: Exception =>
        logger.error(s"âŒ Erreur dans la couche Bronze: ${e.getMessage}")
        throw e
    }
  }
}