package layers

import org.apache.spark.sql.SparkSession
import utils.{Reader, Writer}

/**
 * Couche Bronze - Ingestion des donnÃ©es brutes
 */
object BronzeLayer {

  /**
   * ExÃ©cution de la couche Bronze
   */
  def run()(implicit spark: SparkSession): Boolean = {
    println("ğŸ¥‰ DÃ©marrage couche Bronze")

    val sources = List(
      ("restaurants.csv", "src/data/bronze/restaurants", "CSV"),
      ("restaurant-menus.csv", "src/data/bronze/menus", "CSV")
    )

    val results = sources.map { case (source, output, sourceType) =>
      processSource(source, output, sourceType)
    }

    val success = results.forall(identity)

    if (success) println("âœ… Couche Bronze terminÃ©e")
    else println("âŒ Ã‰chec couche Bronze")

    success
  }

  /**
   * Traitement gÃ©nÃ©rique d'une source de donnÃ©es
   */
  private def processSource(source: String, output: String, sourceType: String)(implicit spark: SparkSession): Boolean = {
    println(s"ğŸ“– Ingestion: $source")

    try {
      val df = sourceType match {
        case "CSV" => Reader.sourceFromCsv(source)
        case "POSTGRES" => Reader.sourceFromPostgreSQL(source)
        case _ => throw new IllegalArgumentException(s"Type de source non supportÃ©: $sourceType")
      }

      df.cache()
      println(s"ğŸ“Š $source: ${df.count()} lignes")

      val success = Writer.writeToParquet(df, output)

      if (success) println(s"âœ… $source â†’ $output")
      else println(s"âŒ Ã‰chec Ã©criture $source")

      df.unpersist()
      success

    } catch {
      case e: Exception =>
        println(s"âŒ Erreur $source: ${e.getMessage}")
        false
    }
  }
}
