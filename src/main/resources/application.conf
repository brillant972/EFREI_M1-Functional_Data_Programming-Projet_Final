# ====================================================================================
# CONFIGURATION SPARK - UBER EATS RESTAURANT ANALYSIS PROJECT
# ====================================================================================

# Configuration Spark principale
spark {
  app {
    name = "uber-eats-restaurant-analysis"
  }
  
  # Configuration du driver et executor
  driver {
    memory = "2g"
    maxResultSize = "1g"
  }
  
  executor {
    memory = "2g"
    cores = 2
  }
  
  # Configuration Spark SQL
  sql {
    adaptive.enabled = true
    adaptive.coalescePartitions.enabled = true
    adaptive.coalescePartitions.minPartitionNum = 1
    adaptive.coalescePartitions.initialPartitionNum = 200
  }
  
  # Configuration de sérialisation
  serializer = "org.apache.spark.serializer.KryoSerializer"
  
  # Niveau de log
  logLevel = "WARN"
}

# Configuration PostgreSQL
database {
  url = "jdbc:postgresql://localhost:54876/uber_eats_analysis"
  user = "postgres"
  password = "postgres"
  driver = "org.postgresql.Driver"
  
  # Configuration de connexion
  connection {
    maxPoolSize = 10
    minPoolSize = 2
    acquireIncrement = 1
    maxStatements = 100
  }
}

# Configuration des chemins de données
data {
  sources {
    restaurants = "restaurants.csv"
    menuItems = "restaurant-menus.csv"
  }
  
  # Chemins de destination (architecture médaillon)
  destinations {
    bronze = "src/data/bronze"
    silver = "src/data/silver" 
    gold = "src/data/gold"
  }
  
  # Formats de fichiers
  formats {
    input = "csv"
    output = "parquet"
    
    # Options CSV
    csv {
      header = true
      inferSchema = true
      delimiter = ","
      quote = "\""
      escape = "\\"
      multiline = false
    }
    
    # Options Parquet
    parquet {
      compression = "snappy"
      mode = "overwrite"
    }
  }
}

# Configuration des tables PostgreSQL
tables {
  ratingsHistory = "restaurant_ratings_history"
  deliveryPerformance = "delivery_performance" 
  promotionalCampaigns = "promotional_campaigns"
}

# Configuration logging
logging {
  level = "INFO"
  pattern = "%d{yyyy-MM-dd HH:mm:ss} %-5level %logger{36} - %msg%n"
  
  # Logs spécifiques
  loggers {
    spark = "WARN"
    akka = "WARN"
    kafka = "WARN"
  }
}

# Configuration business
business {
  # Échantillonnage pour tests
  sampling {
    enabled = false
    fraction = 0.1
  }
  
  # Validation des données
  validation {
    enabled = true
    failOnError = false
  }
  
  # Métriques et monitoring
  metrics {
    enabled = true
    showProgress = true
  }
}
